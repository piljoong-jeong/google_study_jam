{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: import TF and other libraries\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: download Shakespeare dataset\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] length of text: 1115394 characters\n",
      "text[:250]='First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n'\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# NOTE: read the data\n",
    "\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(f\"[DEBUG] length of text: {len(text)} characters\")\n",
    "print(f\"{text[:250]=}\")\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f\"{len(vocab)} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n",
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
     ]
    }
   ],
   "source": [
    "# NOTE: vectorize the text\n",
    "\n",
    "example_texts = [\"abcdefg\", \"xyz\"]\n",
    "\n",
    "# string to set of chars\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
    "print(chars)\n",
    "\n",
    "# set of chars to numerical representation\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)\n",
    "\n",
    "# numerical representation to set of chars\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), \n",
    "    invert=True, \n",
    "    mask_token=None\n",
    ")\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars)\n",
    "\n",
    "# set of chars to string\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ids=<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# NOTE: the prediction task: \n",
    "# given sequence of character, \n",
    "# what is the most probable next character?\n",
    "\n",
    "# NOTE: create training examples and targets\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "print(f\"{all_ids=}\")\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))\n",
    "\n",
    "# batching\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length+1) # NOTE: training set; given sequence(seq_length), predict next character(+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Input:\t b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "[DEBUG] Target:\t b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# NOTE: preparing dataset: (input, label)\n",
    "# `input`: current character\n",
    "# `label`: next character\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "\n",
    "    return input_text, target_text\n",
    "split_input_target(list(\"Tensorflow\"))\n",
    "\n",
    "dataset = sequences.map(split_input_target) # NOTE: transformation using input function\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f\"[DEBUG] Input:\\t {text_from_ids(input_example).numpy()}\")\n",
    "    print(f\"[DEBUG] Target:\\t {text_from_ids(target_example).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# NOTE: create training batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000 # NOTE: size of container which will store already shuffled dataset\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ") # NOTE: pack shuffled data into batches\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: build the model\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # NOTE: layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units, \n",
    "            return_sequences=True, \n",
    "            return_state=True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, \n",
    "             inputs, \n",
    "             states=None, \n",
    "             return_state=False, \n",
    "             training=False):\n",
    "        \n",
    "        x = self.embedding(inputs, training=training)\n",
    "\n",
    "        # NOTE: if training, load & use previous state\n",
    "        if None is states:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        return (x, states) if return_state else x\n",
    "    \n",
    "model = MyModel(\n",
    "    # NOTE: be sure the vocabulary size matches the `StringLookup` layers\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()), \n",
    "    embedding_dim=embedding_dim, \n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "[DEBUG] sampled_indices=array([36, 27, 43, 34, 37, 52, 39, 14, 61, 40, 47, 25, 37, 29, 20, 34,  0,\n",
      "       42, 38,  7, 34, 61, 63, 52, 41, 36, 60,  9, 44, 64, 55,  3, 55,  2,\n",
      "       17, 17, 40, 36, 36, 18,  1, 57,  6, 57, 31, 27, 51, 42, 40, 53, 58,\n",
      "        0,  3, 32, 16, 42, 28, 12, 46, 20, 20,  5, 54, 14,  5, 60, 35, 29,\n",
      "       50, 54, 47, 24, 42, 15,  5, 31, 18,  4, 25, 17, 22, 25,  9, 53, 32,\n",
      "       14, 17, 21, 38, 33, 55,  1, 58,  1,  6, 38, 37, 40, 22, 53])\n",
      "\n",
      "[DEBUG] input: \n",
      "b'you were past all shame,--\\nThose of your fact are so--so past all truth:\\nWhich to deny concerns more'\n",
      "[DEBUG] next char predictions: \n",
      "b\"WNdUXmZAvahLXPGU[UNK]cY,UvxmbWu.eyp!p DDaWWE\\nr'rRNlcans[UNK]!SCcO;gGG&oA&uVPkohKcB&RE$LDIL.nSADHYTp\\ns\\n'YXaIn\"\n"
     ]
    }
   ],
   "source": [
    "# NOTE: validate the model\n",
    "\n",
    "# NOTE: 1. check the shape of the output:\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(f\"{example_batch_predictions.shape} # (batch_size, sequence_length, vocab_size)\")\n",
    "print()\n",
    "\n",
    "# NOTE: 2. layer architecture\n",
    "print(f\"{model.summary()}\")\n",
    "print()\n",
    "\n",
    "# NOTE: 3. get predictions using random\n",
    "# NOTE: using softmax for choosing next best prediction will result stuck in a loop!\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(f\"[DEBUG] {sampled_indices=}\")\n",
    "print()\n",
    "print(f\"[DEBUG] input: \\n{text_from_ids(input_example_batch[0]).numpy()}\")\n",
    "print(f\"[DEBUG] next char predictions: \\n{text_from_ids(sampled_indices).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] prediction shape: (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "[DEBUG] mean loss: 4.191465377807617\n",
      "66.1196060180664\n"
     ]
    }
   ],
   "source": [
    "# NOTE: attach an optimizer & loss function\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(f\"[DEBUG] prediction shape: {example_batch_predictions.shape} # (batch_size, sequence_length, vocab_size)\")\n",
    "print(f\"[DEBUG] mean loss: {example_batch_mean_loss}\")\n",
    "\n",
    "\"\"\"\n",
    "    A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. \n",
    "    To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. \n",
    "    A much higher loss means the model is sure of its wrong answers, and is badly initialized:\n",
    "\"\"\"\n",
    "print(f\"{tf.exp(example_batch_mean_loss).numpy()}\")\n",
    "\n",
    "# NOTE: configure the training procedure\n",
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: configure checkpoints\n",
    "\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, \n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 8s 38ms/step - loss: 2.5620\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 8s 39ms/step - loss: 2.2947\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 2.1414\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.9963\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.8649\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.7529\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 8s 39ms/step - loss: 1.6613\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.5881\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 8s 39ms/step - loss: 1.5305\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 8s 39ms/step - loss: 1.4790\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.4389\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.4002\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.3670\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.3383\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 8s 38ms/step - loss: 1.3073\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.2771\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.2511\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.2246\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 8s 39ms/step - loss: 1.1935\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.1664\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 8s 38ms/step - loss: 1.1332\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 8s 38ms/step - loss: 1.0966\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.0643\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 8s 38ms/step - loss: 1.0270\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.9880\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.9450\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.9010\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 8s 38ms/step - loss: 0.8581\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.8207\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.7815\n"
     ]
    }
   ],
   "source": [
    "# NOTE: execute the training\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:O:\n",
      "Why foolish being now might you ade-hard for frost?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I'll vantage you have gold.\n",
      "\n",
      "BIONDELLO:\n",
      "O heavens! O heavens! O house! have made mine heart\n",
      "Of what is well deserved to have light encounter;\n",
      "Chather Geothest things of me.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, what take thou overth, Pruntium queen,\n",
      "A centage of his father;\n",
      "And whatsoever you will remember what with kings\n",
      "And harsh had heards me to to pass.\n",
      "\n",
      "POLIXENES:\n",
      "I cannot thou mintless sky,\n",
      "Far beheld the other of a woman:\n",
      "And Bolingbroke forget to fawn upon thy\n",
      "fault, nor never was care so far off\n",
      "And rack many hours my tongue nor drinking fair.\n",
      "Farewell: for 'tis worse!\n",
      "\n",
      "SICINIUS:\n",
      "O poison to submising land;\n",
      "To silk, any methought I thank you farth.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now take the son, the fortune spirit is in thine own motion.\n",
      "Where, for an ass come to keep the body that would not have?\n",
      "\n",
      "KING HENRY VI:\n",
      "Can I can holy heart, my son Edward not with hum.\n",
      "To have her honour to the king, and who hooks\n",
      "Mercilither me to lose his reco \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "[DEBUG] run time: 4.748628377914429\n"
     ]
    }
   ],
   "source": [
    "# NOTE: generate text\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 chars_from_ids, \n",
    "                 ids_from_chars, \n",
    "                 temperature=1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # NOTE: create a mask to prevent `[UNK]` from being generated\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # NOTE: put -inf for each bad index\n",
    "            values=[-float(\"inf\")] * len(skip_ids), \n",
    "            indices=skip_ids, \n",
    "            # NOTE: match the shape to the vocabulary, though this is sparse and filtered\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "        return\n",
    "    \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "\n",
    "        # NOTE: convert strings to token IDs\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # NOTE: run the model\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, \n",
    "            states=states, \n",
    "            return_state=True\n",
    "        ) # `predicted_logits`: [batch, char, next_char_logits]\n",
    "\n",
    "        # NOTE: only use the last prediction <- why?\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        predicted_logits = predicted_logits + self.prediction_mask # this way, unwanted index becomes `-inf`\n",
    "\n",
    "        # NOTE: sample the output logits, to generate token IDs\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # NOTE: convert from token to chars\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        return predicted_chars, states\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "# NOTE: run this in a loop to generate some text\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print()\n",
    "print(f\"[DEBUG] run time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:umbrace did\n",
      "Against me of your bearding. Mine uncountry's face,\n",
      "Whose mighty mowhat, even your honour,\n",
      "Out of her wonderful. Whose lady and her's dost mother?\n",
      "I'll keep me from this born, forget thee from force.\n",
      "Nay, no more deep war, what he should we din,\n",
      "And make a posses of that word, we must to thee?\n",
      "\n",
      "COMINIUS:\n",
      "Carbas thee well.\n",
      "\n",
      "BIANCA:\n",
      "Why, no; he, now to thine own.\n",
      "\n",
      "CLAUDIO:\n",
      "'Tis wrong to tender for what thou hast?\n",
      "\n",
      "STANLEY:\n",
      "Unfrain the war, who crown'd with knighting Bolingbroke\n",
      "As Valpids that stabber happiness that you make\n",
      "Your torgue with false foe: I am going\n",
      "Altht home. But what?\n",
      "\n",
      "BRUTUS:\n",
      "O heavens!\n",
      "O woe for valiant, great son and happy mothers!\n",
      "Farewell the rumbless where no mortal compalase,\n",
      "And by the king's shapen made a general lamentatio;\n",
      "for left no pardly with thy womb hath hed us,\n",
      "In fair proud quarrel, to quistisure grows,\n",
      "And we allow their sovereign pland.\n",
      "Dare-tell me, lack Warwick, I am too study\n",
      "To say but harm look' to a king,\n",
      "Kate, softend-hardings; sho \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "[DEBUG] batched run time: 4.619322776794434\n"
     ]
    }
   ],
   "source": [
    "# NOTE: batched text generation\n",
    "\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print()\n",
    "print(f\"[DEBUG] batched run time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f3c9b9b8090>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:isteman.\n",
      "\n",
      "CAPULET:\n",
      "And Poase this tome; I will soon abused good.\n",
      "I'll quench the party of your love.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: export the generator\n",
    "\n",
    "tf.saved_model.save(one_step_model, \"one_step\")\n",
    "one_step_reloaded = tf.saved_model.load(\"one_step\")\n",
    "\n",
    "# validation\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_study_jam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
